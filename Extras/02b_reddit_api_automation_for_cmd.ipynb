{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "        # citing : Sara Soueidan - Project 3, Tim Book - API Code; Zoom recording DSIR-824 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "# APi\n",
    "import requests\n",
    "\n",
    "# automating - use for time based operations\n",
    "import time\n",
    "import datetime\n",
    "import sys\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a api function,  to automate the process\n",
    "# define function, with subreddit, # of times function should run \n",
    "def get_posts(subreddit, n_iter, epoch_right_now):\n",
    "    # store base url\n",
    "    base_url = 'https://api.pushshift.io/reddit/search/comment/?subreddit='\n",
    "    # create empty list\n",
    "    df_list = []\n",
    "    # set epoch to current time\n",
    "    current_time = epoch_right_now\n",
    "    #set up for loop\n",
    "    for post in range(n_iter):\n",
    "        # get requests\n",
    "        res = requests.get(\n",
    "            # base url for response variable\n",
    "            base_url,\n",
    "            # parameters for response\n",
    "            params = {\n",
    "                # subreddit\n",
    "                'subreddit' : subreddit,\n",
    "                # size\n",
    "                'size'      : 80,\n",
    "                # lang == True\n",
    "                'lang'      : True,\n",
    "                # before this time pull everything\n",
    "                'before'    : current_time} # close parameters\n",
    "        ) # close .get\n",
    "        # take data from most recent request, store as df\n",
    "        df = pd.DataFrame(res.json()['data'])\n",
    "        # pull specific columns for df\n",
    "        df = df[['subreddit', 'body', 'comment_type', 'created_utc', 'author', 'permalink']]\n",
    "        # create submission_title column\n",
    "        link_list = list(df['permalink'].str.split('/'))\n",
    "        sub_list = []\n",
    "        for i in link_list:\n",
    "            sub_list.append(i[5])\n",
    "        df['submission_title'] = sub_list\n",
    "        #drop permalink column\n",
    "        df.drop(columns='permalink', inplace=True)\n",
    "        # append to empty dataframe list\n",
    "        df_list.append(df)\n",
    "        # set current time counter back to last epoch in recently appended df\n",
    "        current_time = df['created_utc'].min()\n",
    "        #set sleep time\n",
    "        time.sleep(60)\n",
    "    #return one df for all requests\n",
    "    return pd.concat(df_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create large df of both subreddits\n",
    "reddit_df = pd.concat([get_posts('sportsbook', 1, 1616950893), get_posts('dfsports', 1, 1614777568)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output to csv\n",
    "reddit_df.to_csv('Comments/reddit_comments_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get minimum of previous file\n",
    "df = pd.read_csv('Comments/reddit_comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600, 7)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note:\n",
    "The cells below were used to find new minimums for created_utc.  I could only pull a certain number of comments at one time.  This process needed to be completed 4 time to get to minimum threshold.  These new mins were transfered to .py file to be ran in cmd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['sportsbook', 'dfsports'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['subreddit'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_book = df['subreddit'] == 'sportsbook'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1616950893"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[filter_book]['created_utc'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_dfs = df['subreddit'] == 'dfsports'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1614777568"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[filter_dfs]['created_utc'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get minimum of previous file\n",
    "df1 = pd.read_csv('Comments/reddit_comments_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600, 7)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['sportsbook', 'dfsports'], dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['subreddit'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_book = df1['subreddit'] == 'sportsbook'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1616910248"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1[filter_book]['created_utc'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_dfs = df1['subreddit'] == 'dfsports'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1613755574"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1[filter_dfs]['created_utc'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get minimum of previous file\n",
    "df2 = pd.read_csv('Comments/reddit_comments_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600, 7)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['sportsbook', 'dfsports'], dtype=object)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2['subreddit'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_book = df2['subreddit'] == 'sportsbook'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1616901098"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2[filter_book]['created_utc'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_dfs = df2['subreddit'] == 'dfsports'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1612889923"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2[filter_dfs]['created_utc'].min()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
